a \documentclass[a4paper,11pt]{report}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[francais,english]{babel}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{blindtext}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{lscape}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{hyperref}

%supprime le retrait de paragraphe
\parindent=0em

\newcommand{\Ecole}{}
\newcommand{\Filiere}{Ingénierie Logicielle}
\newcommand{\Cours}{}
\newcommand{\Titre}{}
\newcommand{\Lieu}{}
\newcommand{\ReferentA}{Elena Mugellini}
\newcommand{\PartA}{Sylvain Julmy}
\newcommand{\PartB}{}

\newcommand{\Parts}{\PartA \\ \PartB}
\newcommand{\Referents}{\ReferentA \\ \ReferentB \\ \ReferentC \\ \ReferentD}

%Code macro
\newcommand{\Code}[3]{\lstinputlisting[numbers=left,linerange={#2-#3}]{#1}}

\pagestyle{fancy}
\lhead[]{Julmy,Kalberer}
\chead[]{\Cours}
\rhead[]{\Lieu, le \today}

\setlength{\headheight}{14pt}
\setlength{\headsep}{14pt}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

%supprime les mot "Chapitre N" dans le document
\titleformat{\chapter}{}{\bf\LARGE\thechapter. \space}{0em}{\bf\LARGE}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{tabsize=4,
    basicstyle=\scriptsize,
    %upquote=true,
    aboveskip={1.5\baselineskip},
    columns=fixed,
    showstringspaces=false,
    extendedchars=true,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=single,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    identifierstyle=\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color[rgb]{0,0.6,0},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
    backgroundcolor=\color{white},
    language=Matlab,
    morekeywords={append,select}
    literate=
    {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
    {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
    {à}{{\`a}}1 {è}{{\'e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
    {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
    {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
    {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
    {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
    {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
    {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
    {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
    {€}{{\EUR}}1 {£}{{\pounds}}1
}


\begin{document}

\begin{titlepage}
    \begin{center}

        % only works if a paragraph has started.
        \includegraphics[width=0.15\textwidth]{/home/Snipy/Images/Save/logo_eia.png}~\\[1cm]
        \textsc{\LARGE \Ecole}
        \textsc{\Large \Filiere}\\[1.5cm]
        \textsc{\Large \Cours}\\[0.5cm]

        % Title
        \HRule \\[0.4cm]
        { \huge \bfseries \Titre\\[0.4cm] }
        \HRule \\[1.5cm]

        % Author and supervisor
        \begin{minipage}[t]{0.4\textwidth}
            \begin{flushleft} \large
                \emph{Auteurs:}\\ \Parts
            \end{flushleft}
        \end{minipage}
        \begin{minipage}[t]{0.4\textwidth}
            \begin{flushright} \large
                \emph{Superviseurs:}\\\Referents
            \end{flushright}
        \end{minipage}~\\[1.5cm]

        \begin{center}
            \includegraphics[scale=0.4]{/home/Snipy/Images/Save/logo_algo_gen.jpg}
        \end{center}

        \vfill

        % Bottom of the pages21
        {\large \Lieu, le \today}

    \end{center}
\end{titlepage}

\section*{Q1 : Accuracy rate with data on Moodle}
Nous obtenons un \textit{accuracy rate} de $68.33333\%$

\section*{Q2 : MajorityVoting function in tie case}
En cas d'égalité, c'est le premier élément qui apparait dans la liste qui donne son label au nouveau point.
\begin{center}
    \includegraphics[scale=0.5]{fig1.png}
\end{center}

Voici le code utilisé pour tester :
\begin{lstlisting}
trainData  = [-1,-1;1,1;-1,1;1,-1];
trainLabel = [1;2;3;4];
testData   = [0,0;0,1];
testLabel  = [0,0];

[~,resLabel] = kNN(trainData,trainLabel,testData,2);
figure;
scatter(trainData(:,1),trainData(:,2),[],trainLabel,'*','Linewidth',3);
hold on;
scatter(testData(:,1),testData(:,2),[],resLabel,'x','Linewidth',3);
\end{lstlisting}

\section*{Q3 : Different result with \textit{fitcknn} and \textit{predict}}
Oui, les \textit{accuracy rate} obtenus en utilisant ces deux méthodes sont égaux à $33.33\%$ peut importe le $k$.
\begin{lstlisting}
function [test_label1,test_label2] = kNN(train_set,train_label,test_set,k)

% compute the index (index of the matrix train_data) of the k nearest element to each point 
% IDX = knnsearch(train_set,test_set,'k',k);

mdl = fitcknn(train_set,train_label,'NumNeighbors',k);
IDX = predict(mdl,test_set);

% access and compute each label that's are the most represented
test_label1 = train_label(transpose(mode(IDX,2)));              % just a test tto find the max...
test_label2 = transpose(majorityVoting(train_label,IDX));
end
\end{lstlisting}

\section*{Q4-Q5-Q6 : Best result with euclidean distance, mahalanobis and comparaison}

Pour Euclide, le meilleur score est de $78.33\%1$ à l'index 11 (en bleu) et pour mahalanobis, il s'agit des indexs 10 et 12 
avec un score de $80\%$.

\begin{center}
    \includegraphics[scale=0.29]{fig2.png}
\end{center}

On voit que la distance de mahalanobis est plus représentative que la distance euclidienne en utilisant nos données. Mahalanobis 
prend en compte la répartition des données. Si on avais utilisé des données avec une distribution aléatoire, euclide aurait été 
tout aussi efficace.

\section*{Q7 : Extra}

Après de nombreux essais, nous n'avons pas réussi à implémenter le \textit{MajorityVoting2}, une erreur subsiste dans le code 
ci-desosus. Voici le code produit :
\begin{lstlisting}
function winnerVector = majorityVotingWeighted(test_set,trainData,trainLabel,n_index)

n_classe = length(unique(trainLabel));
score_classe = zeros(n_classe,1);
[numberOfTestData, k] = size(n_index);
winnerVector = zeros(1,numberOfTestData);
for ii = 1:size(test_set,1)
crt_point = test_set(ii);    
distances = pdist2(crt_point,trainData(n_index(1,ii)));
labels = trainLabel(n_index);

pres_label = unique(labels);
for jj = 1:size(pres_label,1)
crt_label = pres_label(jj);
crt_score = distances(labels == crt_label);
score_classe(crt_label) = sum(1 ./ crt_score);
end
[~,winnerVector(ii)] = max(score_classe);
end

end
\end{lstlisting}



\end{document}

